{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968e315a-f432-442d-9359-5c3c29e3e1ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae9dd2e-031e-4a57-8c7a-9705d315e9cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as T\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from pyspark.sql.functions import from_utc_timestamp, to_utc_timestamp, lit, date_format, to_timestamp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import timedelta\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from math import pi, sin, cos\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84307798-2791-45c1-a6e6-e13ca87f5698",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48208ebd-1365-4764-82a1-02bda35732a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Desired resolution of the downsampled power consumption data.\n",
    "\"\"\"\n",
    "res = \"60S\" \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "We set the values for window_length and values_to_predict, which is the low-resolution value divided by the high-resolution value.\n",
    "\"\"\"\n",
    "high_res_freq = 2\n",
    "low_res_freq = 10\n",
    "window_length = int(low_res_freq/high_res_freq)\n",
    "values_to_predict = int(low_res_freq/high_res_freq)\n",
    "\n",
    "\"\"\"\n",
    "We determine the length of the train, validation and test sets, by setting number of rows in each set.\n",
    "\"\"\"\n",
    "train_rows = 4320\n",
    "validation_rows = 1440\n",
    "test_rows = 1440\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Setting the batch_size value.\n",
    "\"\"\"\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Setting the number of epochs and learning rate value.\n",
    "\"\"\"\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parameter values for the T2SR framework.\n",
    "\"\"\"\n",
    "input_size = 1\n",
    "output_size = values_to_predict\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "dropout_p = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d16fdd4d-fef4-4dcf-b03d-35ef48a05379",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example data \n",
    "\n",
    "* In the initial experiment we use real-world data obtained from building in Norway. We create some example data to illustrate the format of the data.\n",
    "\n",
    "* We create 5 days of power consumption data with a 2-second resolution.\n",
    "\n",
    "* We also simulate missing data in order to get the example data as realistic as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e0b8f3c-19eb-4e1c-bf63-c3c27c349113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp       value\n",
      "0 2023-01-01 00:00:00+00:00  119.443437\n",
      "1 2023-01-01 00:00:02+00:00  174.756214\n",
      "2 2023-01-01 00:00:04+00:00  125.664476\n",
      "3 2023-01-01 00:00:06+00:00  213.255499\n",
      "4 2023-01-01 00:00:10+00:00  155.417678\n",
      "5 2023-01-01 00:00:12+00:00  224.596200\n",
      "6 2023-01-01 00:00:14+00:00  226.341309\n",
      "7 2023-01-01 00:00:16+00:00  260.997628\n",
      "8 2023-01-01 00:00:18+00:00  113.396241\n",
      "9 2023-01-01 00:00:20+00:00  235.358306\n",
      "410400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Base date\n",
    "start_date = datetime.strptime(\"2023-01-01T00:00:00.000+0000\", \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "# Generate timestamps with 2-second resolution for 10 days\n",
    "# Correctly calculate the total number of 2-second intervals in 10 days\n",
    "total_intervals = 5 * 24 * 60 * 60 // 2  # Dividing total seconds in 10 days by 2\n",
    "timestamps = [start_date + timedelta(seconds=2*i) for i in range(total_intervals)]\n",
    "\n",
    "# Generate random power consumption values\n",
    "power_consumption = np.random.uniform(low=50, high=300, size=len(timestamps))\n",
    "\n",
    "\n",
    "# Simulate missing values by removing a percentage of data points\n",
    "missing_indices = np.random.choice(len(timestamps), size=int(len(timestamps) * 0.05), replace=False)\n",
    "timestamps = [timestamps[i] for i in range(len(timestamps)) if i not in missing_indices]\n",
    "power_consumption = [power_consumption[i] for i in range(len(power_consumption)) if i not in missing_indices]\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"timestamp\": timestamps,\n",
    "    \"value\": power_consumption\n",
    "})\n",
    "\n",
    "print (df.head(10))\n",
    "\n",
    "print (df.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0147fdc-3c88-4f1e-9ed4-8d463039e788",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "* The data preprocessing is performed in the 'data_preprocessing' notebook. We call on this this notebook by using the %run command.\n",
    "* We then create a function 'final_preprocessed_data' where we call the methods created in 'data_preprocessing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d6ba3b-3fdd-415b-acf1-6c60954c4cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \" /path/to/data_preprocessing\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcef391-f977-408e-9936-85e9f5037d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def final_preprocessed_data (df):\n",
    "    \n",
    "    # Get the interpolated high-resolution data \n",
    "    high_res_data = interpolate_and_fill_missing(df)\n",
    "    \n",
    "    # Get the downsampled low-resolution data\n",
    "    low_res_data  = downsample_data(high_res_data, res)\n",
    "    \n",
    "    # Map the low-resolution and high-resolution data together \n",
    "    mapped_data = mapping_function (high_res_data, low_res_data, window_length)\n",
    "    \n",
    "    # Split into train, validation and test sets\n",
    "    train_df, valid_df, test_df = data_splitting (mapped_data, train_rows, validation_rows, test_rows)\n",
    "    \n",
    "    # Split into feature and targets\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = split_feature_target(train_df, valid_df, test_df)\n",
    "    \n",
    "    # Scale the data\n",
    "    X_train, y_train, X_valid, y_valid, X_test, y_test = scale_data(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, valid_loader, test_loader = to_dataloaders((X_train, y_train), (X_valid, y_valid), (X_test, y_test))\n",
    "\n",
    "    # Returns three dataloaders: train, validation and test\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, valid_loader, test_loader = final_preprocessed_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ddb199-45b0-44b2-8955-56d83e269386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Implementation of T2SR Framework\n",
    "\n",
    "* We implement the architecture of the T2SR framework in the 'model' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "441a566f-3b75-4b0d-946f-ec4b1106bb1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \" /path/to/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16cfeb74-1943-4bd0-8fdb-a1e91bf45b7f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "* We implement functions for training, validation and testing in the 'train' notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c5ea3df-a17d-411b-9eaa-4eea587cec7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \" /path/to/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fcfa90b-47fc-4496-934a-4cbf14bc6668",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fff48244-58fc-4cc6-b8aa-fc4e5137b347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize lists to store loss and MAE for each epoch for training and validation phases\n",
    "train_losses_mse = []\n",
    "train_maes = []\n",
    "valid_losses_mse = []\n",
    "valid_maes = []\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the best validation MSE to infinity for later comparison\n",
    "best_valid_mse = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "# Ensure the model is moved to the GPU if available, otherwise it stays on the CPU\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Start training over specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train the model for one epoch, capturing the mean squared error (MSE) and mean absolute error (MAE)\n",
    "    train_loss, train_mae = train_model(model, optimizer, criterion_mse, train_loader, device)\n",
    "    # Store the training loss and MAE for this epoch\n",
    "    train_losses_mse.append(train_loss)\n",
    "    train_maes.append(train_mae)\n",
    "    \n",
    "    # Print the training loss and MAE for the current epoch\n",
    "    print(f\"Train MSE: {train_loss:.4f}, Train MAE: {train_mae:.4f}\")\n",
    "\n",
    "    # Validate the model using the validation dataset, capturing the MSE and MAE\n",
    "    valid_loss, valid_mae = validate_model(model, criterion_mse, valid_loader, device)\n",
    "    # Store the validation loss and MAE for this epoch\n",
    "    valid_losses_mse.append(valid_loss)\n",
    "    valid_maes.append(valid_mae)\n",
    "    \n",
    "    # Print the validation loss and MAE for the current epoch\n",
    "    print(f\"Valid MSE: {valid_loss:.4f}, Valid MAE: {valid_mae:.4f}\")\n",
    "\n",
    "    # If the current validation loss (MSE) is lower than the best one recorded, update the best score and save the model\n",
    "    if valid_loss < best_valid_mse:\n",
    "        best_valid_mse = valid_loss  # Update the best validation MSE\n",
    "        # Save the current model's state dictionary as the best model checkpoint\n",
    "        torch.save(model.state_dict(), 'best_model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42dcf5d9-9ce0-4bcd-b983-6825f3a3934c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Postprocessing of Data\n",
    "* We must perform some postprocessing of the data, includign reshaping data and inverse transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8fedd4-f867-4fa2-9ac8-2263ee4ace08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_mae, test_predictions, test_targets = evaluate_model(model, criterion_mse, test_loader, device)\n",
    "\n",
    "\n",
    "# Convert predictions list to numpy array\n",
    "predictions_array = np.array(test_predictions)\n",
    "targets_array = np.array(test_targets)\n",
    "\n",
    "\n",
    "# Reshape your data\n",
    "predictions_reshaped = predictions_array.reshape(-1, values_to_predict)\n",
    "targets_reshaped = targets_array.reshape(-1, values_to_predict)\n",
    "\n",
    "# Inverse transform\n",
    "predicted_values_original_scale = scaler_target.inverse_transform(predictions_reshaped)\n",
    "actual_values_original_scale = scaler_target.inverse_transform(targets_reshaped)\n",
    "\n",
    "# If you want to flatten it further, you can do so:\n",
    "predicted_values_original_scale_flattened = predicted_values_original_scale.flatten()\n",
    "actual_values_original_scale_flattened = actual_values_original_scale.flatten()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e760cc58-1785-44a1-9e57-e3f529231bfd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Model Evaluation\n",
    "\n",
    "* Finally we are able to calculate MAE and MSE and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "723a3a56-aa1a-4c25-b2e5-fda9a1a0211f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate MAE and MSE using NumPy\n",
    "\n",
    "#MAE\n",
    "mae_original = np.mean(np.abs(actual_values_original_scale_flattened - predicted_values_original_scale_flattened))\n",
    "\n",
    "#MSE\n",
    "mse_original = np.mean((actual_values_original_scale_flattened - predicted_values_original_scale_flattened)**2)\n",
    "\n",
    "\n",
    "\n",
    "# Print MAE and MSE in original units\n",
    "print(\"MAE in original units:\", mae_original)\n",
    "print(\"MSE in original units:\", mse_original)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
