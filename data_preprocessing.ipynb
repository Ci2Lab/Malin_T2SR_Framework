{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336b3531-a24c-4ba6-980c-348d8867fd08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PIPELINE FOR THE DATA PREPROCESSING\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "Filling in missing data with linear interpolation in the high-resolution power concumption data.\n",
    "\n",
    "Returns dataframe with consumption values and timestamps.\n",
    "\"\"\"\n",
    "\n",
    "def interpolate_and_fill_missing(df):\n",
    "\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Set the 'timestamp' column as the DataFrame index\n",
    "    df_copy.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Resample the DataFrame with the desired frequency (2-second interval) and fill missing timestamps\n",
    "    df_resampled = df_copy.resample('2S').asfreq()\n",
    "\n",
    "    # Interpolate missing values (NaN) in the 'value' column using linear interpolation\n",
    "    df_resampled['value'] = df_resampled['value'].interpolate(method='linear')\n",
    "\n",
    "    # Reset the index to make 'timestamp' a column again\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "\n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "Downsamples the high-resolution data data into the desired low-resolution data\n",
    "\n",
    "Returns dataframe with consumption values and timestamps.\n",
    "\"\"\"    \n",
    "    \n",
    "def downsample_data(df, res):\n",
    "\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    df_resampled = df.copy()\n",
    "    \n",
    "    # Convert the 'timestamp' column to datetime format\n",
    "    df_resampled['timestamp'] = pd.to_datetime(df_resampled['timestamp'])\n",
    "    \n",
    "    # Set the 'timestamp' column as the DataFrame index\n",
    "    df_resampled.set_index(\"timestamp\", inplace=True)\n",
    "\n",
    "    # Resampling resolution using the mean\n",
    "    df_resampled = df_resampled.resample(res).agg({'value': 'mean'})\n",
    "\n",
    "    # Resetting index to make it a column again\n",
    "    df_resampled.reset_index(inplace=True)\n",
    "    \n",
    "    return df_resampled\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Function for mapping each high-resolution value to the desired low-resolution sequence.\n",
    "\n",
    "Returns dataframe containing low-resolution values and its corresponding high-resolution sequences.\n",
    "\"\"\"\n",
    "\n",
    "def mapping_function(low_res_df, high_res_df, window_length):\n",
    "    # Ensure the dataframes are sorted by timestamp in ascending order\n",
    "    low_res_dataframe = low_res_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "    high_res_dataframe = high_res_df.sort_values(by=\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "    # Convert the timestamps to unix time for easy lookup\n",
    "    low_res_dataframe[\"timestamp_unix\"] = (\n",
    "        pd.to_datetime(low_res_dataframe[\"timestamp\"]).astype(int) // 10**9\n",
    "    )\n",
    "    high_res_dataframe[\"timestamp_unix\"] = (\n",
    "        pd.to_datetime(high_res_dataframe[\"timestamp\"]).astype(int) // 10**9\n",
    "    )\n",
    "\n",
    "    # Convert data to Numpy arrays for faster computations\n",
    "    low_res_timestamps = low_res_dataframe[\"timestamp_unix\"].values\n",
    "    low_res_feature = low_res_dataframe.drop(columns=[\"timestamp\", \"timestamp_unix\"]).values\n",
    "\n",
    "    high_res_timestamps = high_res_dataframe[\"timestamp_unix\"].values\n",
    "    high_res_values = high_res_dataframe[\"value\"].values\n",
    "\n",
    "    # Create a lookup dictionary mapping timestamps to all low_res features\n",
    "    low_res_lookup = dict(zip(low_res_timestamps, low_res_feature))\n",
    "\n",
    "    # Initialize empty lists to store the high-resolution data (H) and low-resolution data (L)\n",
    "    H_data = []\n",
    "    L_data = []\n",
    "\n",
    "    # Sliding window over the high-resolution dataframe\n",
    "    for i in range(len(high_res_dataframe) - window_length + 1):\n",
    "        # Extract the high-temporal-resolution data sequence in the sliding window \n",
    "        H_sequence_values = high_res_values[i : i + window_length]\n",
    "        H_sequence_timestamps = high_res_timestamps[i : i + window_length]\n",
    "        \n",
    "        # Extract the corresponding low-temporal-resolution data point using the first timestamp in H_sequence\n",
    "        L_features = low_res_lookup.get(H_sequence_timestamps[0], None)\n",
    "\n",
    "        # Check if L_features exists and append it along with high_res data\n",
    "        if L_features is not None:\n",
    "            H_data.append(H_sequence_values)\n",
    "            L_data.append(L_features)\n",
    "\n",
    "    # Convert the lists to pandas Series\n",
    "    H_series = pd.Series(H_data[:-2])  # Drop the last x rows to match lengths\n",
    "    L_series = pd.Series(L_data[:-2])  # Drop the last x rows to match lengths\n",
    "\n",
    "    # Create a new dataframe containing the mapped data\n",
    "    mapped_df= pd.DataFrame({\"H_sequence\": H_series, \"L_feature\": L_series})\n",
    "\n",
    "    return mapped_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Splits the dataframe into train, validation and test sets.\n",
    "\n",
    "Returns three dataframes: train, validation and test\n",
    "\"\"\"\n",
    "\n",
    "def data_splitting(df, train_rows, validation_rows, test_rows):\n",
    "    \n",
    "    # Ensure the total number of rows for train and validation does not exceed the dataframe length\n",
    "    assert train_rows + validation_rows+test_rows <= len(df), \"Sum of train and validation rows exceeds dataframe length\"\n",
    "\n",
    "    # Use the first predetermined number of rows for train set\n",
    "    train_df = df[:train_rows]\n",
    "    \n",
    "    # Use the next predetermined number of rows for validation set\n",
    "    valid_df = df[train_rows:train_rows + validation_rows]\n",
    "    \n",
    "    # Use the next predetermined number of rows for test set\n",
    "    test_df = df[train_rows + validation_rows:train_rows + validation_rows + test_rows]\n",
    "\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Splits train, validation and test sets into feature and target.\n",
    "\n",
    "Returns six lists: a feature list and a target list for each of the datasets.\n",
    "\"\"\"\n",
    "\n",
    "def split_feature_target (train_data, valid_data, test_data):\n",
    "\n",
    "    # Splitting train dataset\n",
    "    X_train = train_data[\"L_feature\"].tolist()  # Convert pandas series of lists into list of lists\n",
    "    y_train = train_data[\"H_sequence\"].tolist()  # Convert pandas series of lists into list of lists\n",
    "\n",
    "    # Splitting validation dataset\n",
    "    X_valid = valid_data[\"L_feature\"].tolist()\n",
    "    y_valid = valid_data[\"H_sequence\"].tolist()\n",
    "\n",
    "    # Splitting test dataset\n",
    "    X_test = test_data[\"L_feature\"].tolist()\n",
    "    y_test = test_data[\"H_sequence\"].tolist()\n",
    "\n",
    "    return  X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Scaling the input and target data with StandardScaler.\n",
    "\n",
    "Returns six lists.\n",
    "\"\"\"\n",
    "\n",
    "def scale_data (X_train, y_train, X_valid, y_valid, X_test, y_test):\n",
    "\n",
    "    # Instantiate separate scalers for input and the target\n",
    "    scaler_input = StandardScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "\n",
    "    # Fit the scalers using training data\n",
    "    scaler_input.fit(X_train)\n",
    "    scaler_target.fit(y_train) \n",
    "\n",
    "\n",
    "    # Transform the input and targets for train, validation, and test sets\n",
    "\n",
    "    for dataset in [X_train, X_valid, X_test]:\n",
    "        for x in dataset:\n",
    "            x[0] = scaler_input.transform([[x[0]]])[0][0]\n",
    "\n",
    "    y_train = [scaler_target.transform([y])[0] for y in y_train]\n",
    "    y_valid = [scaler_target.transform([y])[0] for y in y_valid]\n",
    "    y_test = [scaler_target.transform([y])[0] for y in y_test]\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Converts pairs of inputs (X) and targets (y) into their respective dataloaders.\n",
    "\n",
    "Returns a list of dataloaders corresponding to each (X, y) pair.\n",
    "\"\"\"\n",
    "def to_dataloaders(*dataset_pairs):\n",
    "\n",
    "    dataloaders = []\n",
    "    for X, y in dataset_pairs:\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32, requires_grad=True)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size, shuffle=False)\n",
    "        \n",
    "        dataloaders.append(dataloader)\n",
    "    \n",
    "    return dataloaders\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_preprocessing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
